{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "URL = os.getenv(\"URL\")\n",
    "USER_ID = os.getenv(\"USER_ID\")\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import requests\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"Name\", \"PremiereDate\", \"CriticRating\", \"OfficialRating\", \"Overview\", \"Taglines\",\n",
    "    \"Genres\", \"CommunityRating\", \"RunTimeTicks\", \"ProductionYear\", \"People\", \"Studios\", \"UserData\"\n",
    "]\n",
    "\n",
    "class JellyfinClient(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, url, user_id, api_key, played_status=\"IsPlayed\", limit=None):\n",
    "        \"\"\"\n",
    "        Initializes the Jellyfin Client.\n",
    "        :param url: Base URL of the Jellyfin server.\n",
    "        :param user_id: User ID for the API request.\n",
    "        :param api_key: API key for authentication.\n",
    "        :param played_status: \"IsPlayed\" or \"IsNotPlayed\" to filter movies.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.user_id = user_id\n",
    "        self.api_key = api_key\n",
    "        self.played_status = played_status  # Switch for IsPlayed or IsNotPlayed\n",
    "        self.limit = limit\n",
    "\n",
    "    def get_amount(self):\n",
    "        \"\"\"Fetch the total number of movies with the current filter.\"\"\"\n",
    "        res = requests.get(\n",
    "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
    "            f\"?StartIndex=0&Limit=1&Recursive=true&IncludeItemTypes=Movie\"\n",
    "            f\"&api_key={self.api_key}&Filters={self.played_status}\"\n",
    "        )\n",
    "        res.raise_for_status()\n",
    "        return res.json().get(\"TotalRecordCount\")\n",
    "\n",
    "    def get_chunk(self, start, chunk):\n",
    "        \"\"\"Fetch a chunk of movies with the current filter.\"\"\"\n",
    "        res = requests.get(\n",
    "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
    "            f\"?StartIndex={start}&Limit={chunk}&Recursive=true&IncludeItemTypes=Movie\"\n",
    "            f\"&api_key={self.api_key}&Filters={self.played_status}\"\n",
    "            f\"&Fields=Budget,Genres,Overview,People,Revenue,Studios,Taglines,ProviderIds,\"\n",
    "            f\"CriticRating,OfficialRating,PremiereDate,CommunityRating,RunTimeTicks,ProductionYear,UserData\"\n",
    "        )\n",
    "        res.raise_for_status()\n",
    "        return res.json().get(\"Items\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        \"\"\"Fetch all movies in chunks based on the current filter.\"\"\"\n",
    "        size = self.limit or self.get_amount()\n",
    "\n",
    "        start = 0\n",
    "        chunk = 50\n",
    "        all_movies = []\n",
    "        while start < size:\n",
    "            print(\n",
    "                f\"Fetching movies {start} - {start + chunk} of {size} ({self.played_status})\")\n",
    "            all_movies.extend(self.get_chunk(start, chunk))\n",
    "            start += chunk\n",
    "        return all_movies\n",
    "\n",
    "\n",
    "class MovieDataCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_keep):\n",
    "        self.columns_to_keep = columns_to_keep\n",
    "\n",
    "    def filter_movie(self, movie):\n",
    "        \"\"\"Filter and clean a single movie dictionary.\"\"\"\n",
    "        filtered = {key: value for key,\n",
    "                    value in movie.items() if key in self.columns_to_keep}\n",
    "\n",
    "        # Process nested People and Studios\n",
    "        if \"People\" in filtered:\n",
    "            filtered[\"People\"] = [[person.get(\"Id\"), person.get(\n",
    "                \"Type\")] for person in filtered[\"People\"]]\n",
    "        if \"Studios\" in filtered:\n",
    "            filtered[\"Studios\"] = [studio[\"Id\"]\n",
    "                                   for studio in filtered[\"Studios\"] if \"Name\" in studio]\n",
    "        if \"UserData\" in filtered:\n",
    "            filtered[\"IsFavorite\"] = filtered[\"UserData\"].get(\"IsFavorite\")\n",
    "            del filtered[\"UserData\"]\n",
    "\n",
    "        if \"Taglines\" in filtered:\n",
    "            filtered[\"Taglines\"] = (\"\\n\").join(filtered[\"Taglines\"])\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame([self.filter_movie(movie) for movie in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching movies 0 - 50 of 5 (IsPlayed)\n"
     ]
    }
   ],
   "source": [
    "data_pipeline = Pipeline([\n",
    "    (\"jellyfin_client\", JellyfinClient(URL, USER_ID, API_KEY, played_status=\"IsPlayed\", limit=5)),\n",
    "    (\"data_cleaner\", MovieDataCleaner(columns_to_keep)),\n",
    "])\n",
    "\n",
    "dp = data_pipeline.fit_transform(None)\n",
    "# dp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Name has null: False\n",
      "Column: PremiereDate has null: False\n",
      "Column: CriticRating has null: True\n",
      "Column: OfficialRating has null: True\n",
      "Column: Overview has null: False\n",
      "Column: Taglines has null: False\n",
      "Column: Genres has null: False\n",
      "Column: CommunityRating has null: False\n",
      "Column: RunTimeTicks has null: True\n",
      "Column: ProductionYear has null: False\n",
      "Column: People has null: False\n",
      "Column: Studios has null: False\n",
      "Column: IsFavorite has null: False\n"
     ]
    }
   ],
   "source": [
    "# dp.columns\n",
    "# for col in dp.columns: \n",
    "#     print(f'Column: {col} has null: {dp[col].isnull().values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, OneHotEncoder\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class MovieFeatureEngineerWithFastText(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.fasttext_name_model = None\n",
    "        self.fasttext_overview_model = None\n",
    "        self.fasttext_people_model = None\n",
    "        self.fasttext_studios_model = None\n",
    "\n",
    "        # Encoders and scalers\n",
    "        self.ohe_rating = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "        self.mlb_genres = MultiLabelBinarizer()\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def train_fasttext_model(self, text_data, vector_size=100):\n",
    "        # Save temporary data\n",
    "        temp_filename = 'temp_fasttext.txt'\n",
    "        text_data.dropna().to_csv(temp_filename, index=False, header=False)\n",
    "        # Train the model\n",
    "        model = fasttext.train_unsupervised(temp_filename, model='skipgram', dim=vector_size)\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit numerical scaler\n",
    "        X_filled = X[['CriticRating', 'CommunityRating', 'RunTimeTicks', 'ProductionYear']].fillna(-7)\n",
    "        self.scaler.fit(X_filled)\n",
    "\n",
    "        # Train FastText models\n",
    "        self.fasttext_name_model = self.train_fasttext_model(X['Name'].fillna(\"unknown\"))\n",
    "        self.fasttext_overview_model = self.train_fasttext_model(X['Overview'].fillna(\"unknown\"))\n",
    "        self.fasttext_people_model = self.train_fasttext_model(\n",
    "            X['People'].apply(lambda p: ' '.join([item[0] for item in p if isinstance(item, list)])).fillna(\"unknown\")\n",
    "        )\n",
    "        self.fasttext_studios_model = self.train_fasttext_model(X['Studios'].fillna(\"unknown\"))\n",
    "\n",
    "        # Fit encoders for categorical and multilabel features\n",
    "        self.ohe_rating.fit(X[['OfficialRating']].fillna(\"unknown\"))\n",
    "        self.mlb_genres.fit(X['Genres'].fillna(\"unknown\"))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "\n",
    "        # Transform numerical features\n",
    "        numerical_features = ['CriticRating', 'CommunityRating', 'RunTimeTicks', 'ProductionYear']\n",
    "        df[numerical_features] = df[numerical_features].fillna(-7)\n",
    "        scaled_numerical = csr_matrix(self.scaler.transform(df[numerical_features]))\n",
    "\n",
    "        # Helper function for FastText embeddings\n",
    "        def get_fasttext_vector(text, model):\n",
    "            if isinstance(text, str):\n",
    "                vectors = [model.get_word_vector(word) for word in text.split() if word in model.words]\n",
    "                if vectors:\n",
    "                    return np.mean(vectors, axis=0)\n",
    "            return np.zeros(model.get_dimension())\n",
    "\n",
    "        # Generate FastText embeddings\n",
    "        name_embeddings = np.vstack(df['Name'].fillna(\"unknown\").apply(lambda x: get_fasttext_vector(x, self.fasttext_name_model)))\n",
    "        overview_embeddings = np.vstack(df['Overview'].fillna(\"unknown\").apply(lambda x: get_fasttext_vector(x, self.fasttext_overview_model)))\n",
    "        people_embeddings = np.vstack(\n",
    "            df['People'].apply(\n",
    "                lambda p: get_fasttext_vector(\n",
    "                    ' '.join([item[0] for item in p if isinstance(item, list)]), self.fasttext_people_model)\n",
    "            )\n",
    "        )\n",
    "        studios_embeddings = np.vstack(df['Studios'].fillna(\"unknown\").apply(lambda x: get_fasttext_vector(x, self.fasttext_studios_model)))\n",
    "\n",
    "        # Debug: Check dimensions of all embeddings\n",
    "        print(f\"Name embeddings shape: {name_embeddings.shape}\")\n",
    "        print(f\"Overview embeddings shape: {overview_embeddings.shape}\")\n",
    "        print(f\"People embeddings shape: {people_embeddings.shape}\")\n",
    "        print(f\"Studios embeddings shape: {studios_embeddings.shape}\")\n",
    "\n",
    "        # Convert FastText embeddings (dense arrays) to sparse format\n",
    "        name_embeddings_sparse = csr_matrix(name_embeddings)\n",
    "        overview_embeddings_sparse = csr_matrix(overview_embeddings)\n",
    "        people_embeddings_sparse = csr_matrix(people_embeddings)\n",
    "        studios_embeddings_sparse = csr_matrix(studios_embeddings)\n",
    "\n",
    "        # Count role occurrences\n",
    "        def count_roles(people):\n",
    "            role_counter = Counter()\n",
    "            for person in people:\n",
    "                if isinstance(person, list) and len(person) > 1:\n",
    "                    role_counter[person[1]] += 1\n",
    "            return role_counter\n",
    "\n",
    "        role_counts = df['People'].apply(count_roles)\n",
    "        role_features = csr_matrix(np.array([\n",
    "            [role_counts.get('Actor', -7),\n",
    "             role_counts.get('Director', -7),\n",
    "             role_counts.get('Writer', -7),\n",
    "             role_counts.get('Producer', -7)]\n",
    "            for role_counts in role_counts\n",
    "        ]))\n",
    "\n",
    "        # Date encoding\n",
    "        df[\"PremiereDate\"] = pd.to_datetime(df['PremiereDate'])\n",
    "        df['year'] = df['PremiereDate'].dt.year.astype(int)\n",
    "        df['month'] = df['PremiereDate'].dt.month.astype(int)\n",
    "        df['day'] = df['PremiereDate'].dt.day.astype(int)\n",
    "        df['day_of_week'] = df['PremiereDate'].dt.dayofweek.astype(int)\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        df['quarter'] = df['PremiereDate'].dt.quarter.astype(int)\n",
    "        df['week_of_year'] = df['PremiereDate'].dt.isocalendar().week.astype(int)\n",
    "        df['day_of_year'] = df['PremiereDate'].dt.dayofyear.astype(int)\n",
    "        df = df.drop(columns=['PremiereDate'])\n",
    "        date_features = csr_matrix(df[['year', 'month', 'day', 'day_of_week', 'is_weekend', 'quarter', 'week_of_year', 'day_of_year']].values)\n",
    "\n",
    "        # Encode categorical and multilabel features\n",
    "        official_rating_encoded = self.ohe_rating.transform(df[['OfficialRating']].fillna(\"unknown\"))\n",
    "        genres_encoded = csr_matrix(self.mlb_genres.transform(df['Genres'].fillna(\"unknown\")))\n",
    "\n",
    "        # Stack all features together\n",
    "        final_sparse_matrix = hstack([\n",
    "            scaled_numerical,               # Scaled numerical data\n",
    "            name_embeddings_sparse,         # Sparse FastText embeddings for Name\n",
    "            overview_embeddings_sparse,     # Sparse FastText embeddings for Overview\n",
    "            people_embeddings_sparse,       # Sparse FastText embeddings for People\n",
    "            studios_embeddings_sparse,      # Sparse FastText embeddings for Studios\n",
    "            role_features,                  # Sparse role count features\n",
    "            date_features,                  # Sparse date encoding features\n",
    "            official_rating_encoded,        # Sparse one-hot encoded Official Rating\n",
    "            genres_encoded                  # Sparse multi-label binarized Genres\n",
    "        ])\n",
    "\n",
    "        return final_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('feature_engineer', MovieFeatureEngineer()),\n",
    "])\n",
    "\n",
    "# Split data\n",
    "df = dp.drop(columns=['IsFavorite'])\n",
    "y = dp['IsFavorite']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply pipeline and resampling\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)  # Use transform instead of fit_transform for test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('feature_engineer', MovieFeatureEngineer()),\n",
    "])\n",
    "\n",
    "# Split data\n",
    "df = dp.drop(columns=['IsFavorite'])\n",
    "y = dp['IsFavorite']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply pipeline and resampling\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)  # Use transform instead of fit_transform for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [200, 500, 1000, 2000, 4000],          # Number of trees in the forest\n",
    "    'max_depth': [10, 20, 50, None],           # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],           # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],             # Minimum samples required at each leaf node\n",
    "    'max_features': ['sqrt', 'log2', None],    # Number of features considered for the best split\n",
    "    'bootstrap': [True, False],                # Use bootstrap samples\n",
    "    'criterion': ['gini', 'entropy'],          # Split quality criterion\n",
    "}\n",
    "\n",
    "# Model and scoring\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    scoring=recall_scorer,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "tuned_rf = random_search.best_estimator_\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Predict probabilities for AUC evaluation\n",
    "y_pred_proba = tuned_rf.predict_proba(X_test_transformed)[:, 1]\n",
    "y_pred = tuned_rf.predict(X_test_transformed)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline = Pipeline([\n",
    "    (\"jellyfin_client\", JellyfinClient(URL, USER_ID, API_KEY, played_status=\"IsUnPlayed\")),\n",
    "    (\"data_cleaner\", MovieDataCleaner(columns_to_keep)),\n",
    "    \n",
    "])\n",
    "\n",
    "new_df = new_pipeline.fit_transform(None)\n",
    "X_new_transformed = full_pipeline.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Edge\n",
      "Operation Fortune: Ruse de Guerre\n",
      "Black Hawk Down\n",
      "Beverly Hills Cop: Axel F\n",
      "Snowden\n",
      "Adventureland\n",
      "Wind River\n",
      "Silence\n",
      "The Bikeriders\n",
      "Love Actually\n",
      "Bound by Honor\n",
      "End of Watch\n",
      "Fallen\n",
      "House on Haunted Hill\n",
      "The Infiltrator\n",
      "Angel Has Fallen\n",
      "Punch-Drunk Love\n",
      "Strange Darling\n",
      "Stillwater\n",
      "To Die For\n",
      "LEGO Scooby-Doo! Haunted Hollywood\n",
      "Public Enemies\n",
      "Pearl Harbor\n",
      "Con Air\n",
      "12 Strong\n",
      "Hit Man\n",
      "True Romance\n",
      "The Dreamers\n",
      "Unforgiven\n",
      "The Fly\n",
      "Natural Born Killers\n",
      "2 Guns\n",
      "Arbitrage\n",
      "The Frighteners\n",
      "The Crow\n",
      "The Report\n",
      "The Strangers: Prey at Night\n",
      "MaXXXine\n",
      "Carlito's Way\n",
      "Rounders\n",
      "Fifty Shades Freed\n",
      "Longlegs\n",
      "LEGO Scooby-Doo! Blowout Beach Bash\n",
      "The Killing of a Sacred Deer\n",
      "I Origins\n",
      "Triangle of Sadness\n",
      "Cold Mountain\n",
      "Tombstone\n",
      "Bound\n",
      "Bad Boys for Life\n",
      "Affliction\n",
      "The House That Jack Built\n",
      "Resident Evil: Death Island\n",
      "American Psycho\n",
      "The Lighthouse\n",
      "The Big Short\n",
      "The Firm\n",
      "Bad Boys: Ride or Die\n",
      "Legend\n",
      "Expend4bles\n",
      "Conspiracy Theory\n",
      "White Boy Rick\n",
      "Broken City\n",
      "The Accountant\n",
      "The Devil's Advocate\n",
      "G.I. Jane\n"
     ]
    }
   ],
   "source": [
    "predictions = tuned_rf.predict(X_new_transformed)\n",
    "predicted_probabilities = tuned_rf.predict_proba(X_new_transformed)[:, 1]\n",
    "\n",
    "results_df = new_df[['Name']].copy()  # or 'MovieID', if you have it\n",
    "results_df['PredictedIsFavorite'] = predictions\n",
    "results_df['ProbabilityIsFavorite'] = predicted_probabilities\n",
    "\n",
    "# results_df\n",
    "print((\"\\n\").join(results_df[results_df['ProbabilityIsFavorite'] > 0.6]['Name']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
