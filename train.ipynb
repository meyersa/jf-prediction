{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZX3wSrOWe3q",
        "outputId": "72aafa16-65c5-45d2-ecb0-171807c12277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 4)) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: fasttext-wheel in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 6)) (0.9.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fasttext-wheel->-r requirements.txt (line 6)) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fasttext-wheel->-r requirements.txt (line 6)) (63.2.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QpB6oFgIWD8i"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv(\".env\")\n",
        "\n",
        "URL = os.getenv(\"URL\")\n",
        "USER_ID = os.getenv(\"USER_ID\")\n",
        "# USER_ID = \"d7252e24a2a34cab83e147e26fcee5d8\"\n",
        "\n",
        "API_KEY = os.getenv(\"API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "OQNfw7fuWD8k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import requests\n",
        "\n",
        "columns_to_keep = [\n",
        "    \"Name\",\n",
        "    \"PremiereDate\",\n",
        "    \"CriticRating\",\n",
        "    \"OfficialRating\",\n",
        "    \"Overview\",\n",
        "    \"Taglines\",\n",
        "    \"Genres\",\n",
        "    \"CommunityRating\",\n",
        "    \"RunTimeTicks\",\n",
        "    \"ProductionYear\",\n",
        "    \"People\",\n",
        "    \"Studios\",\n",
        "    \"UserData\",\n",
        "    \"CollectionName\",\n",
        "    \"CollectionSize\",\n",
        "    \"CollectionAge\",\n",
        "    \"CollectionOrder\",\n",
        "    \"FirstInCollection\",\n",
        "    \"AgeSinceLastMovie\"\n",
        "]\n",
        "\n",
        "\n",
        "class JellyfinClient(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, url, user_id, api_key, played_status=\"IsPlayed\", limit=None):\n",
        "        \"\"\"\n",
        "        Initializes the Jellyfin Client.\n",
        "        :param url: Base URL of the Jellyfin server.\n",
        "        :param user_id: User ID for the API request.\n",
        "        :param api_key: API key for authentication.\n",
        "        :param played_status: \"IsPlayed\" or \"IsNotPlayed\" to filter movies.\n",
        "        \"\"\"\n",
        "        self.url = url\n",
        "        self.user_id = user_id\n",
        "        self.api_key = api_key\n",
        "        self.played_status = played_status  # Switch for IsPlayed or IsNotPlayed\n",
        "        self.limit = limit\n",
        "\n",
        "    def get_movies(self):\n",
        "        \"\"\"Fetch all movies based on the current filter\"\"\"\n",
        "        size = self.limit or self.get_movie_amount()\n",
        "\n",
        "        start = 0\n",
        "        chunk = 50\n",
        "        all_movies = []\n",
        "\n",
        "        while start < size:\n",
        "            print(\n",
        "                f\"Fetching Movie {start} - {start + chunk} of {size} ({self.played_status})\"\n",
        "            )\n",
        "            all_movies.extend(self.get_movie_chunk(start, chunk))\n",
        "            start += chunk\n",
        "\n",
        "        return all_movies\n",
        "\n",
        "    def get_movie_amount(self):\n",
        "        \"\"\"Fetch the total number of movies with the current filter.\"\"\"\n",
        "        res = requests.get(\n",
        "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
        "            f\"?StartIndex=0&Limit=1&Recursive=true&IncludeItemTypes=Movie\"\n",
        "            f\"&api_key={self.api_key}&Filters={self.played_status}\"\n",
        "        )\n",
        "        res.raise_for_status()\n",
        "        return res.json().get(\"TotalRecordCount\")\n",
        "\n",
        "    def get_movie_chunk(self, start, chunk):\n",
        "        \"\"\"Fetch a chunk of movies with the current filter.\"\"\"\n",
        "        res = requests.get(\n",
        "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
        "            f\"?StartIndex={start}&Limit={chunk}&Recursive=true&IncludeItemTypes=Movie\"\n",
        "            f\"&api_key={self.api_key}&Filters={self.played_status}\"\n",
        "            f\"&Fields=Budget,Genres,Overview,People,Revenue,Studios,Taglines,ProviderIds,\"\n",
        "            f\"CriticRating,OfficialRating,PremiereDate,CommunityRating,RunTimeTicks,ProductionYear,UserData\"\n",
        "        )\n",
        "        res.raise_for_status()\n",
        "        return res.json().get(\"Items\")\n",
        "\n",
        "    def get_collections(self):\n",
        "        \"\"\"Fetch all collections and their child movies.\"\"\"\n",
        "        size = self.get_collection_amount()\n",
        "        start = 0\n",
        "        chunk = 50\n",
        "        all_collections = {}\n",
        "\n",
        "        while start < size:\n",
        "            chunk_data = self.get_collection_chunk(start, chunk)\n",
        "            for collection in chunk_data:\n",
        "                collection_id = collection[\"Id\"]\n",
        "                collection_name = collection[\"Name\"]\n",
        "                children = self.get_collection_children(collection_id)\n",
        "                all_collections[collection_id] = {\n",
        "                    \"Name\": collection_name,\n",
        "                    \"Movies\": [\n",
        "                        {\n",
        "                            \"Id\": child[\"Id\"],\n",
        "                            \"ProductionYear\": child.get(\"ProductionYear\", 0),\n",
        "                            \"Name\": child.get(\"Name\"),\n",
        "                        }\n",
        "                        for child in children\n",
        "                    ],\n",
        "                }\n",
        "            start += chunk\n",
        "\n",
        "        for collection_data in all_collections.values():\n",
        "            collection_data[\"Movies\"].sort(key=lambda x: x[\"ProductionYear\"])\n",
        "\n",
        "        return all_collections\n",
        "\n",
        "\n",
        "    def get_collection_children(self, collection_id):\n",
        "        res = requests.get(\n",
        "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
        "            f\"?ParentId={collection_id}&Recursive=false\"\n",
        "            f\"&api_key={self.api_key}\"\n",
        "        )\n",
        "        res.raise_for_status()\n",
        "        return res.json().get(\"Items\")\n",
        "\n",
        "    def enrich_movies_with_collections(self, movies, collections):\n",
        "        # Create a dictionary with movie ID as key for quick access\n",
        "        movie_dict = {\n",
        "            movie.get(\"Id\"): {\n",
        "                **movie,  # Unpack existing movie fields\n",
        "                \"InCollection\": False,\n",
        "                \"CollectionName\": None,\n",
        "                \"CollectionSize\": 0,\n",
        "                \"CollectionAge\": 0,\n",
        "                \"CollectionOrder\": None,\n",
        "                \"FirstInCollection\": False,\n",
        "                \"AgeSinceLastMovie\": None,\n",
        "            }\n",
        "            for movie in movies\n",
        "        }\n",
        "\n",
        "        # Update movies with collection-specific data\n",
        "        for collection_id, collection_data in collections.items():\n",
        "            movies_in_collection = collection_data[\"Movies\"]\n",
        "            for order, movie_data in enumerate(movies_in_collection, start=1):\n",
        "                movie_id = movie_data[\"Id\"]\n",
        "                if movie_id in movie_dict:\n",
        "                    movie = movie_dict[movie_id]\n",
        "                    movie[\"InCollection\"] = True\n",
        "                    movie[\"CollectionName\"] = collection_data[\"Name\"]\n",
        "                    movie[\"CollectionSize\"] = len(movies_in_collection)\n",
        "                    movie[\"CollectionAge\"] = (\n",
        "                        movies_in_collection[-1][\"ProductionYear\"] - movies_in_collection[0][\"ProductionYear\"]\n",
        "                    )\n",
        "                    movie[\"CollectionOrder\"] = order\n",
        "                    movie[\"FirstInCollection\"] = order == 1\n",
        "                    if order > 1:\n",
        "                        movie[\"AgeSinceLastMovie\"] = (\n",
        "                            movie_data[\"ProductionYear\"] - movies_in_collection[order - 2][\"ProductionYear\"]\n",
        "                        )\n",
        "\n",
        "        return list(movie_dict.values())\n",
        "        \n",
        "    def get_collection_amount(self):\n",
        "        \"\"\"Fetch the total number of collections\"\"\"\n",
        "        res = requests.get(\n",
        "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
        "            f\"?StartIndex=0&Limit=1&Recursive=true&IncludeItemTypes=BoxSet\"\n",
        "            f\"&api_key={self.api_key}\"\n",
        "        )\n",
        "        res.raise_for_status()\n",
        "        return res.json().get(\"TotalRecordCount\")\n",
        "\n",
        "    def get_collection_chunk(self, start, chunk):\n",
        "        \"\"\"Fetch a chunk of movies with the current filter.\"\"\"\n",
        "        res = requests.get(\n",
        "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
        "            f\"?StartIndex={start}&Limit={chunk}&Recursive=true&IncludeItemTypes=BoxSet\"\n",
        "            f\"&api_key={self.api_key}\"\n",
        "        )\n",
        "        res.raise_for_status()\n",
        "        return res.json().get(\"Items\")\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X=None):\n",
        "        \"\"\"Fetch and enrich all movies.\"\"\"\n",
        "        \n",
        "        all_movies = self.get_movies()\n",
        "        all_collections = self.get_collections()\n",
        "        enriched_movies = self.enrich_movies_with_collections(all_movies, all_collections)\n",
        "        \n",
        "        return enriched_movies\n",
        "\n",
        "\n",
        "class MovieDataCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns_to_keep):\n",
        "        self.columns_to_keep = columns_to_keep\n",
        "\n",
        "    def filter_movie(self, movie):\n",
        "        \"\"\"Filter and clean a single movie dictionary.\"\"\"\n",
        "        filtered = {\n",
        "            key: value for key, value in movie.items() if key in self.columns_to_keep\n",
        "        }\n",
        "\n",
        "        # Process nested People and Studios\n",
        "        if \"People\" in filtered:\n",
        "            filtered[\"People\"] = [person.get(\"Name\") for person in filtered[\"People\"]]\n",
        "            filtered[\"People\"] = filtered[\"People\"][:10]\n",
        "\n",
        "        if \"Studios\" in filtered:\n",
        "            filtered[\"Studios\"] = [studio[\"Name\"] for studio in filtered[\"Studios\"]]\n",
        "            filtered[\"Studios\"] = filtered[\"Studios\"][:5]\n",
        "\n",
        "        if \"Genres\" in filtered:\n",
        "            filtered[\"Genres\"] = filtered[\"Genres\"][:5]\n",
        "\n",
        "        if \"UserData\" in filtered:\n",
        "            filtered[\"IsFavorite\"] = filtered[\"UserData\"].get(\"IsFavorite\")\n",
        "            del filtered[\"UserData\"]\n",
        "\n",
        "        if \"Taglines\" in filtered:\n",
        "            filtered[\"Taglines\"] = (\"\\n\").join(filtered[\"Taglines\"])\n",
        "\n",
        "        # Convert from Microseconds\n",
        "        if \"RunTimeTicks\" in filtered:\n",
        "            filtered[\"LengthInHours\"] = filtered[\"RunTimeTicks\"] / 10000000 / 60 / 60\n",
        "            del filtered[\"RunTimeTicks\"]\n",
        "\n",
        "        return filtered\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return pd.DataFrame([self.filter_movie(movie) for movie in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6rSZE9zWD8l",
        "outputId": "4e8d31e0-9aa3-479d-dcf2-ecc2381a1612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching Movie 0 - 50 of 538 (IsPlayed)\n",
            "Fetching Movie 50 - 100 of 538 (IsPlayed)\n",
            "Fetching Movie 100 - 150 of 538 (IsPlayed)\n",
            "Fetching Movie 150 - 200 of 538 (IsPlayed)\n",
            "Fetching Movie 200 - 250 of 538 (IsPlayed)\n",
            "Fetching Movie 250 - 300 of 538 (IsPlayed)\n",
            "Fetching Movie 300 - 350 of 538 (IsPlayed)\n",
            "Fetching Movie 350 - 400 of 538 (IsPlayed)\n",
            "Fetching Movie 400 - 450 of 538 (IsPlayed)\n",
            "Fetching Movie 450 - 500 of 538 (IsPlayed)\n",
            "Fetching Movie 500 - 550 of 538 (IsPlayed)\n"
          ]
        }
      ],
      "source": [
        "data_pipeline = Pipeline([\n",
        "    (\"jellyfin_client\", JellyfinClient(URL, USER_ID, API_KEY, played_status=\"IsPlayed\")),\n",
        "    (\"data_cleaner\", MovieDataCleaner(columns_to_keep)),\n",
        "])\n",
        "\n",
        "dp = data_pipeline.fit_transform(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Name                  object\n",
              "PremiereDate          object\n",
              "CriticRating         float64\n",
              "OfficialRating        object\n",
              "Overview              object\n",
              "Taglines              object\n",
              "Genres                object\n",
              "CommunityRating      float64\n",
              "ProductionYear         int64\n",
              "People                object\n",
              "Studios               object\n",
              "CollectionName        object\n",
              "CollectionSize         int64\n",
              "CollectionAge          int64\n",
              "CollectionOrder      float64\n",
              "FirstInCollection       bool\n",
              "AgeSinceLastMovie    float64\n",
              "IsFavorite              bool\n",
              "LengthInHours        float64\n",
              "dtype: object"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dp.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "jREfwvQVWD8l",
        "outputId": "9915bde0-d0c7-4846-a7da-0af65e230fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column: Name has null: False\n",
            "Column: PremiereDate has null: False\n",
            "Column: CriticRating has null: True\n",
            "Column: OfficialRating has null: True\n",
            "Column: Overview has null: False\n",
            "Column: Taglines has null: False\n",
            "Column: Genres has null: False\n",
            "Column: CommunityRating has null: True\n",
            "Column: ProductionYear has null: False\n",
            "Column: People has null: False\n",
            "Column: Studios has null: False\n",
            "Column: CollectionName has null: True\n",
            "Column: CollectionSize has null: False\n",
            "Column: CollectionAge has null: False\n",
            "Column: CollectionOrder has null: True\n",
            "Column: FirstInCollection has null: False\n",
            "Column: AgeSinceLastMovie has null: True\n",
            "Column: IsFavorite has null: False\n",
            "Column: LengthInHours has null: True\n"
          ]
        }
      ],
      "source": [
        "for col in dp.columns:\n",
        "    print(f'Column: {col} has null: {dp[col].isnull().values.any()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "gf8k_ylEWD8m"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, OneHotEncoder\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from collections import Counter\n",
        "\n",
        "class MovieFeatureEngineerWithFastText(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100):\n",
        "        self.vector_size = vector_size\n",
        "        self.fasttext_combined_model = None\n",
        "        \n",
        "        # Encoders and scalers\n",
        "        self.ohe_rating = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
        "        self.ohe_col_name = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
        "\n",
        "        self.mlb_genres = MultiLabelBinarizer()\n",
        "        self.mlb_people = MultiLabelBinarizer()\n",
        "        self.mlb_studios = MultiLabelBinarizer()\n",
        "\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def load_pretrained_fasttext_model(self):\n",
        "        \"\"\"\n",
        "        Load a pre-trained FastText model.\n",
        "        \"\"\"\n",
        "        fasttext.util.download_model('en', if_exists='ignore')  # Downloads Common Crawl vectors\n",
        "        model = fasttext.load_model('cc.en.300.bin')\n",
        "        return model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the feature engineering pipeline.\n",
        "        \"\"\"\n",
        "        # Fit numerical scaler\n",
        "        X_filled = X[['CriticRating', 'CommunityRating', 'LengthInHours', 'ProductionYear', 'CollectionSize', 'CollectionAge', 'AgeSinceLastMovie']].fillna(-7)\n",
        "        self.scaler.fit(X_filled)\n",
        "\n",
        "        # Load the pre-trained FastText model\n",
        "        self.fasttext_combined_model = self.load_pretrained_fasttext_model()\n",
        "\n",
        "        # Ensure `Genres`, `People`, and `Studios` are lists\n",
        "        X['Genres'] = X['Genres'].apply(lambda g: g if isinstance(g, list) else [])\n",
        "        X['People'] = X['People'].apply(lambda p: p if isinstance(p, list) else [])\n",
        "        X['Studios'] = X['Studios'].apply(lambda s: s if isinstance(s, list) else [])\n",
        "\n",
        "        # Fit encoders for categorical and multilabel features\n",
        "        self.ohe_rating.fit(X[['OfficialRating']].fillna(\"unknown\"))\n",
        "        self.ohe_col_name.fit(X[['CollectionName']].fillna(\"unknown\"))\n",
        "        self.mlb_genres.fit(X['Genres'])\n",
        "        self.mlb_people.fit(X['People'])\n",
        "        self.mlb_studios.fit(X['Studios'])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the input DataFrame into feature matrix.\n",
        "        \"\"\"\n",
        "        df = X.copy()\n",
        "\n",
        "        # Transform numerical features\n",
        "        numerical_features = ['CriticRating', 'CommunityRating', 'LengthInHours', 'ProductionYear', 'CollectionSize', 'CollectionAge', 'AgeSinceLastMovie']\n",
        "        df[numerical_features] = df[numerical_features].fillna(-7)\n",
        "        scaled_numerical = csr_matrix(self.scaler.transform(df[numerical_features]))\n",
        "\n",
        "        # Helper function for FastText embeddings\n",
        "        def get_fasttext_vector(text, model):\n",
        "            if isinstance(text, str) and text.strip():\n",
        "                vectors = [model.get_word_vector(word) for word in text.split() if word in model.words]\n",
        "                if vectors:\n",
        "                    return np.mean(vectors, axis=0)\n",
        "            return np.zeros(model.get_dimension())\n",
        "\n",
        "        # Combine text data for embedding generation\n",
        "        combined_text = (\n",
        "            df['Name'].fillna(\"\").astype(str) + \" \" +\n",
        "            df['Overview'].fillna(\"\").astype(str) + \" \" + \n",
        "            df['Taglines'].fillna(\"\").astype(str)\n",
        "        )\n",
        "\n",
        "        combined_embeddings = np.vstack(combined_text.apply(lambda x: get_fasttext_vector(x, self.fasttext_combined_model)))\n",
        "        combined_embeddings_sparse = csr_matrix(combined_embeddings)\n",
        "\n",
        "        # Count role occurrences\n",
        "        def count_roles(people):\n",
        "            role_counter = Counter()\n",
        "            for person in people:\n",
        "                if isinstance(person, list) and len(person) > 1:\n",
        "                    role_counter[person[1]] += 1\n",
        "            return role_counter\n",
        "\n",
        "        role_counts = df['People'].apply(count_roles).fillna(Counter())\n",
        "        role_features = csr_matrix(np.array([\n",
        "            [\n",
        "                role_counts.get('Actor', -7),\n",
        "                role_counts.get('Director', -7),\n",
        "                role_counts.get('Writer', -7),\n",
        "                role_counts.get('Producer', -7)\n",
        "            ]\n",
        "            for role_counts in role_counts\n",
        "        ]))\n",
        "\n",
        "        # Date encoding\n",
        "        df[\"PremiereDate\"] = pd.to_datetime(df['PremiereDate'], errors='coerce')\n",
        "        df['year'] = df['PremiereDate'].dt.year.fillna(-7).astype(int)\n",
        "        df['month'] = df['PremiereDate'].dt.month.fillna(-7).astype(int)\n",
        "        df['day'] = df['PremiereDate'].dt.day.fillna(-7).astype(int)\n",
        "        df['day_of_week'] = df['PremiereDate'].dt.dayofweek.fillna(-7).astype(int)\n",
        "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
        "        df['week_of_year'] = df['PremiereDate'].dt.isocalendar().week.fillna(-7).astype(int)\n",
        "        df['day_of_year'] = df['PremiereDate'].dt.dayofyear.fillna(-7).astype(int)\n",
        "        df = df.drop(columns=['PremiereDate'])\n",
        "        date_features = csr_matrix(df[['year', 'month', 'day', 'is_weekend', 'week_of_year', 'day_of_year']].values)\n",
        "\n",
        "        # Encode categorical and multilabel features\n",
        "        official_rating_encoded = self.ohe_rating.transform(df[['OfficialRating']].fillna(\"unknown\"))\n",
        "        collection_name_encoded = self.ohe_col_name.transform(df[['CollectionName']].fillna(\"none\"))        \n",
        "        genres_encoded = csr_matrix(self.mlb_genres.transform(df['Genres']))\n",
        "        people_encoded = csr_matrix(self.mlb_people.transform(df['People']))\n",
        "        studios_encoded = csr_matrix(self.mlb_studios.transform(df['Studios']))\n",
        "\n",
        "        # Binary Features\n",
        "        binary_features = ['FirstInCollection']\n",
        "        df[binary_features] = df[binary_features].fillna(False).astype(int)\n",
        "        binary_features_sparse = csr_matrix(df[binary_features].values)\n",
        "\n",
        "        # Ordinal Features\n",
        "        ordinal_features = ['CollectionOrder']\n",
        "        df[ordinal_features] = df[ordinal_features].fillna(-1)\n",
        "        ordinal_features_sparse = csr_matrix(df[ordinal_features].values)\n",
        "\n",
        "        # Stack all features together\n",
        "        final_sparse_matrix = hstack([\n",
        "            scaled_numerical,               # Scaled numerical data\n",
        "            combined_embeddings_sparse,     # Unified FastText embeddings\n",
        "            role_features,                  # Role count features\n",
        "            date_features,                  # Date encoding features\n",
        "            official_rating_encoded,        # One-hot encoded Official Rating\n",
        "            collection_name_encoded,        # One-hot encoded Collection Name\n",
        "            genres_encoded,                 # Multi-label binarized Genres\n",
        "            people_encoded,                 # Multi-label binarized People\n",
        "            studios_encoded,                # Multi-label binarized Studios\n",
        "            binary_features_sparse,         # Binary features for first in Collection\n",
        "            ordinal_features_sparse         # Ordinal Features for Collection Order\n",
        "        ])\n",
        "\n",
        "        return final_sparse_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foCL4Zc0WD8m",
        "outputId": "6473790f-dccf-4c27-cd77-8046c96c6097"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD \n",
        "\n",
        "# Split data\n",
        "df = dp.drop(columns=['IsFavorite'])\n",
        "y = dp['IsFavorite']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Full pipeline\n",
        "full_pipeline = Pipeline([\n",
        "    ('feature_engineer', MovieFeatureEngineerWithFastText()),\n",
        "    ('TSVD', TruncatedSVD(n_components=X_train.shape[0]))\n",
        "])\n",
        "\n",
        "# Apply pipeline and resampling\n",
        "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
        "X_test_transformed = full_pipeline.transform(X_test)  # Use transform instead of fit_transform for test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I22g_k4ecaWN",
        "outputId": "817a9dfc-1072-44fd-886b-d31a50ae0b4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(430, 430)"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_transformed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN6az3JsWD8m",
        "outputId": "4ef4cdf8-7813-4607-bc7a-44205d41d05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
            "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 10}\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.6389\n",
            "Precision: 0.5952\n",
            "Recall: 0.9091\n",
            "F1 Score: 0.7194\n",
            "ROC-AUC: 0.8453\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.79      0.36      0.49        53\n",
            "        True       0.60      0.91      0.72        55\n",
            "\n",
            "    accuracy                           0.64       108\n",
            "   macro avg       0.69      0.63      0.61       108\n",
            "weighted avg       0.69      0.64      0.61       108\n",
            "\n",
            "Confusion Matrix:\n",
            "[[19 34]\n",
            " [ 5 50]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2],\n",
        "    'min_samples_leaf': [1],\n",
        "\n",
        "}\n",
        "\n",
        "# Model and scoring\n",
        "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    scoring='f1',\n",
        "    n_iter=20,\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Best model\n",
        "tuned_rf = random_search.best_estimator_\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Predict probabilities for AUC evaluation\n",
        "y_pred_proba = tuned_rf.predict_proba(X_test_transformed)[:, 1]\n",
        "y_pred = tuned_rf.predict(X_test_transformed)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbT-BF3HWD8n",
        "outputId": "1dd28f61-0a20-4557-9ab6-6cbc9bd30ad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching Movie 0 - 50 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 50 - 100 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 100 - 150 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 150 - 200 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 200 - 250 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 250 - 300 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 300 - 350 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 350 - 400 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 400 - 450 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 450 - 500 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 500 - 550 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 550 - 600 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 600 - 650 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 650 - 700 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 700 - 750 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 750 - 800 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 800 - 850 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 850 - 900 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 900 - 950 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 950 - 1000 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1000 - 1050 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1050 - 1100 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1100 - 1150 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1150 - 1200 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1200 - 1250 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1250 - 1300 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1300 - 1350 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1350 - 1400 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1400 - 1450 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1450 - 1500 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1500 - 1550 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1550 - 1600 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1600 - 1650 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1650 - 1700 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1700 - 1750 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1750 - 1800 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1800 - 1850 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1850 - 1900 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1900 - 1950 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 1950 - 2000 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 2000 - 2050 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 2050 - 2100 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 2100 - 2150 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 2150 - 2200 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 2200 - 2250 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 2250 - 2300 of 2322 (IsUnPlayed)\n",
            "Fetching Movie 2300 - 2350 of 2322 (IsUnPlayed)\n"
          ]
        }
      ],
      "source": [
        "new_pipeline = Pipeline([\n",
        "    (\"jellyfin_client\", JellyfinClient(URL, USER_ID, API_KEY, played_status=\"IsUnPlayed\")),\n",
        "    (\"data_cleaner\", MovieDataCleaner(columns_to_keep)),\n",
        "\n",
        "])\n",
        "\n",
        "new_df = new_pipeline.fit_transform(None)\n",
        "X_new_transformed = full_pipeline.transform(new_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xyfcLMcWD8n",
        "outputId": "b01b63f4-17f1-4c2b-eb0b-b163976ff215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Lord of the Rings: The War of the Rohirrim: 0.69\n",
            "Predestination: 0.69\n",
            "Final Destination 5: 0.67\n",
            "Gladiator II: 0.66\n",
            "The Foreigner: 0.65\n",
            "Jerry Maguire: 0.35\n",
            "Collateral: 0.36\n",
            "Racing Stripes: 0.36\n",
            "Chef: 0.36\n",
            "The Butterfly Effect: 0.37\n"
          ]
        }
      ],
      "source": [
        "predictions = tuned_rf.predict(X_new_transformed)\n",
        "predicted_probabilities = tuned_rf.predict_proba(X_new_transformed)[:, 1]\n",
        "\n",
        "results_df = new_df[['Name']].copy()  # or 'MovieID', if you have it\n",
        "results_df['PredictedIsFavorite'] = predictions\n",
        "results_df['ProbabilityIsFavorite'] = predicted_probabilities\n",
        "\n",
        "# Top 5 highest\n",
        "top_5 = results_df.nlargest(5, 'ProbabilityIsFavorite')\n",
        "\n",
        "# Bottom 5 lowest\n",
        "bottom_5 = results_df.nsmallest(5, 'ProbabilityIsFavorite')\n",
        "\n",
        "# Combine the two lists and format the output\n",
        "output = \"\\n\".join(\n",
        "    f\"{row['Name']}: {row['ProbabilityIsFavorite']:.2f}\"\n",
        "    for _, row in pd.concat([top_5, bottom_5]).iterrows()\n",
        ")\n",
        "\n",
        "print(output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
