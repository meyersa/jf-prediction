{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "URL = os.getenv(\"URL\")\n",
    "USER_ID = os.getenv(\"USER_ID\")\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import requests\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"Name\", \"PremiereDate\", \"CriticRating\", \"OfficialRating\", \"Overview\", \"Taglines\",\n",
    "    \"Genres\", \"CommunityRating\", \"RunTimeTicks\", \"ProductionYear\", \"People\", \"Studios\", \"UserData\"\n",
    "]\n",
    "\n",
    "class JellyfinClient(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, url, user_id, api_key, played_status=\"IsPlayed\", limit=None):\n",
    "        \"\"\"\n",
    "        Initializes the Jellyfin Client.\n",
    "        :param url: Base URL of the Jellyfin server.\n",
    "        :param user_id: User ID for the API request.\n",
    "        :param api_key: API key for authentication.\n",
    "        :param played_status: \"IsPlayed\" or \"IsNotPlayed\" to filter movies.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.user_id = user_id\n",
    "        self.api_key = api_key\n",
    "        self.played_status = played_status  # Switch for IsPlayed or IsNotPlayed\n",
    "        self.limit = limit\n",
    "\n",
    "    def get_amount(self):\n",
    "        \"\"\"Fetch the total number of movies with the current filter.\"\"\"\n",
    "        res = requests.get(\n",
    "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
    "            f\"?StartIndex=0&Limit=1&Recursive=true&IncludeItemTypes=Movie\"\n",
    "            f\"&api_key={self.api_key}&Filters={self.played_status}\"\n",
    "        )\n",
    "        res.raise_for_status()\n",
    "        return res.json().get(\"TotalRecordCount\")\n",
    "\n",
    "    def get_chunk(self, start, chunk):\n",
    "        \"\"\"Fetch a chunk of movies with the current filter.\"\"\"\n",
    "        res = requests.get(\n",
    "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
    "            f\"?StartIndex={start}&Limit={chunk}&Recursive=true&IncludeItemTypes=Movie\"\n",
    "            f\"&api_key={self.api_key}&Filters={self.played_status}\"\n",
    "            f\"&Fields=Budget,Genres,Overview,People,Revenue,Studios,Taglines,ProviderIds,\"\n",
    "            f\"CriticRating,OfficialRating,PremiereDate,CommunityRating,RunTimeTicks,ProductionYear,UserData\"\n",
    "        )\n",
    "        res.raise_for_status()\n",
    "        return res.json().get(\"Items\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        \"\"\"Fetch all movies in chunks based on the current filter.\"\"\"\n",
    "        size = self.limit or self.get_amount()\n",
    "\n",
    "        start = 0\n",
    "        chunk = 20\n",
    "        all_movies = []\n",
    "        while start < size:\n",
    "            print(\n",
    "                f\"Fetching movies {start} - {start + chunk} of {size} ({self.played_status})\")\n",
    "            all_movies.extend(self.get_chunk(start, chunk))\n",
    "            start += chunk\n",
    "        return all_movies\n",
    "\n",
    "\n",
    "class MovieDataCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_keep):\n",
    "        self.columns_to_keep = columns_to_keep\n",
    "\n",
    "    def filter_movie(self, movie):\n",
    "        \"\"\"Filter and clean a single movie dictionary.\"\"\"\n",
    "        filtered = {key: value for key,\n",
    "                    value in movie.items() if key in self.columns_to_keep}\n",
    "\n",
    "        # Process nested People and Studios\n",
    "        if \"People\" in filtered:\n",
    "            filtered[\"People\"] = [[person.get(\"Id\"), person.get(\n",
    "                \"Type\")] for person in filtered[\"People\"]]\n",
    "        if \"Studios\" in filtered:\n",
    "            filtered[\"Studios\"] = [studio[\"Id\"]\n",
    "                                   for studio in filtered[\"Studios\"] if \"Name\" in studio]\n",
    "        if \"UserData\" in filtered:\n",
    "            filtered[\"IsFavorite\"] = filtered[\"UserData\"].get(\"IsFavorite\")\n",
    "            del filtered[\"UserData\"]\n",
    "\n",
    "        if \"Taglines\" in filtered:\n",
    "            filtered[\"Taglines\"] = (\"\\n\").join(filtered[\"Taglines\"])\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame([self.filter_movie(movie) for movie in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching movies 0 - 20 of 496 (IsPlayed)\n",
      "Fetching movies 20 - 40 of 496 (IsPlayed)\n",
      "Fetching movies 40 - 60 of 496 (IsPlayed)\n",
      "Fetching movies 60 - 80 of 496 (IsPlayed)\n",
      "Fetching movies 80 - 100 of 496 (IsPlayed)\n",
      "Fetching movies 100 - 120 of 496 (IsPlayed)\n",
      "Fetching movies 120 - 140 of 496 (IsPlayed)\n",
      "Fetching movies 140 - 160 of 496 (IsPlayed)\n",
      "Fetching movies 160 - 180 of 496 (IsPlayed)\n",
      "Fetching movies 180 - 200 of 496 (IsPlayed)\n",
      "Fetching movies 200 - 220 of 496 (IsPlayed)\n",
      "Fetching movies 220 - 240 of 496 (IsPlayed)\n",
      "Fetching movies 240 - 260 of 496 (IsPlayed)\n",
      "Fetching movies 260 - 280 of 496 (IsPlayed)\n",
      "Fetching movies 280 - 300 of 496 (IsPlayed)\n",
      "Fetching movies 300 - 320 of 496 (IsPlayed)\n",
      "Fetching movies 320 - 340 of 496 (IsPlayed)\n",
      "Fetching movies 340 - 360 of 496 (IsPlayed)\n",
      "Fetching movies 360 - 380 of 496 (IsPlayed)\n",
      "Fetching movies 380 - 400 of 496 (IsPlayed)\n",
      "Fetching movies 400 - 420 of 496 (IsPlayed)\n",
      "Fetching movies 420 - 440 of 496 (IsPlayed)\n",
      "Fetching movies 440 - 460 of 496 (IsPlayed)\n",
      "Fetching movies 460 - 480 of 496 (IsPlayed)\n",
      "Fetching movies 480 - 500 of 496 (IsPlayed)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'PremiereDate', 'CriticRating', 'OfficialRating', 'Overview',\n",
       "       'Taglines', 'Genres', 'CommunityRating', 'RunTimeTicks',\n",
       "       'ProductionYear', 'People', 'Studios', 'IsFavorite'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pipeline = Pipeline([\n",
    "    (\"jellyfin_client\", JellyfinClient(URL, USER_ID, API_KEY, played_status=\"IsPlayed\")),\n",
    "    (\"data_cleaner\", MovieDataCleaner(columns_to_keep)),\n",
    "])\n",
    "\n",
    "dp = data_pipeline.fit_transform(None)\n",
    "dp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Name has null: False\n",
      "Column: PremiereDate has null: False\n",
      "Column: CriticRating has null: True\n",
      "Column: OfficialRating has null: True\n",
      "Column: Overview has null: False\n",
      "Column: Taglines has null: False\n",
      "Column: Genres has null: False\n",
      "Column: CommunityRating has null: False\n",
      "Column: RunTimeTicks has null: True\n",
      "Column: ProductionYear has null: False\n",
      "Column: People has null: False\n",
      "Column: Studios has null: False\n",
      "Column: IsFavorite has null: False\n"
     ]
    }
   ],
   "source": [
    "dp.columns\n",
    "for col in dp.columns: \n",
    "    print(f'Column: {col} has null: {dp[col].isnull().values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class MovieFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Text vectorizers for sparse data\n",
    "        self.tfidf_name = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "        self.tfidf_overview = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "        self.tfidf_taglines = TfidfVectorizer(max_features=200, stop_words='english')\n",
    "        self.tfidf_people = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "        \n",
    "        # Encoders and scalers\n",
    "        self.ohe_rating = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "        self.mlb_genres = MultiLabelBinarizer()\n",
    "        self.mlb_studios = MultiLabelBinarizer()\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit numerical scaler\n",
    "        X_filled = X[['CriticRating', 'CommunityRating', 'RunTimeTicks', 'ProductionYear']].fillna(-7)\n",
    "        self.scaler.fit(X_filled)\n",
    "\n",
    "        # Fit text TF-IDF vectorizers\n",
    "        self.tfidf_name.fit(X['Name'].fillna(\"unknown\"))\n",
    "        self.tfidf_overview.fit(X['Overview'].fillna(\"unknown\"))\n",
    "        self.tfidf_taglines.fit(X['Taglines'].fillna(\"unknown\"))\n",
    "        self.tfidf_people.fit(X['People'].apply(lambda p: ' '.join([item[0] for item in p if isinstance(item, list)])).fillna(\"unknown\"))\n",
    "\n",
    "        # Fit encoders for categorical and multilabel features\n",
    "        self.ohe_rating.fit(X[['OfficialRating']].fillna(\"unknown\"))\n",
    "        self.mlb_genres.fit(X['Genres'].fillna(\"unknown\"))\n",
    "        self.mlb_studios.fit(X['Studios'].fillna(\"unknown\"))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Transform numerical features\n",
    "        numerical_features = ['CriticRating', 'CommunityRating', 'RunTimeTicks', 'ProductionYear']\n",
    "        df[numerical_features] = df[numerical_features].fillna(-7)\n",
    "        scaled_numerical = csr_matrix(self.scaler.transform(df[numerical_features]))\n",
    "\n",
    "        # Transform text features to sparse matrices\n",
    "        name_tfidf = self.tfidf_name.transform(df['Name'].fillna(\"unknown\"))\n",
    "        overview_tfidf = self.tfidf_overview.transform(df['Overview'].fillna(\"unknown\"))\n",
    "        taglines_tfidf = self.tfidf_taglines.transform(df['Taglines'].fillna(\"unknown\"))\n",
    "        people_tfidf = self.tfidf_people.transform(\n",
    "            df['People'].apply(lambda p: ' '.join([item[0] for item in p if isinstance(item, list)])).fillna(\"unknown\")\n",
    "        )\n",
    "\n",
    "        # Count role occurrences\n",
    "        def count_roles(people):\n",
    "            role_counter = Counter()\n",
    "            for person in people:\n",
    "                if isinstance(person, list) and len(person) > 1:\n",
    "                    role_counter[person[1]] += 1\n",
    "            return role_counter\n",
    "\n",
    "        role_counts = df['People'].apply(count_roles)\n",
    "        role_features = csr_matrix(np.array([\n",
    "            [role_counts.get('Actor', -7),\n",
    "            role_counts.get('Director', -7),\n",
    "            role_counts.get('Writer', -7),\n",
    "            role_counts.get('Producer', -7)]\n",
    "            for role_counts in role_counts\n",
    "        ]))\n",
    "\n",
    "        # Date encoding\n",
    "        df[\"PremiereDate\"] = pd.to_datetime(df['PremiereDate'])\n",
    "        df['year'] = df['PremiereDate'].dt.year.astype(int)\n",
    "        df['month'] = df['PremiereDate'].dt.month.astype(int)\n",
    "        df['day'] = df['PremiereDate'].dt.day.astype(int)\n",
    "        df['day_of_week'] = df['PremiereDate'].dt.dayofweek.astype(int)\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        df['quarter'] = df['PremiereDate'].dt.quarter.astype(int)\n",
    "        df['week_of_year'] = df['PremiereDate'].dt.isocalendar().week.astype(int)\n",
    "        df['day_of_year'] = df['PremiereDate'].dt.dayofyear.astype(int)\n",
    "        df = df.drop(columns=['PremiereDate'])\n",
    "        date_features = csr_matrix(df[['year', 'month', 'day', 'day_of_week', 'is_weekend', 'quarter', 'week_of_year', 'day_of_year']].values)\n",
    "\n",
    "        # Encode categorical and multilabel features\n",
    "        official_rating_encoded = self.ohe_rating.transform(df[['OfficialRating']].fillna(\"unknown\"))\n",
    "        genres_encoded = csr_matrix(self.mlb_genres.transform(df['Genres'].fillna(\"unknown\")))\n",
    "        studios_encoded = csr_matrix(self.mlb_studios.transform(df['Studios'].fillna(\"unknown\")))\n",
    "\n",
    "        # Stack all sparse matrices together (ensuring everything is sparse)\n",
    "        final_sparse_matrix = hstack([\n",
    "            scaled_numerical,  # Sparse numerical data\n",
    "            name_tfidf,        # Sparse TF-IDF data\n",
    "            overview_tfidf,    # Sparse TF-IDF data\n",
    "            taglines_tfidf,    # Sparse TF-IDF data\n",
    "            people_tfidf,      # Sparse TF-IDF data\n",
    "            role_features,     # Sparse role count features\n",
    "            date_features,     # Sparse date encoding features\n",
    "            official_rating_encoded,  # Sparse one-hot encoded data\n",
    "            genres_encoded,    # Sparse multilabel data\n",
    "            studios_encoded    # Sparse multilabel data\n",
    "        ])\n",
    "\n",
    "        return final_sparse_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 2910)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_pipeline = Pipeline([\n",
    "    (\"feature_engineer\", MovieFeatureEngineer())\n",
    "])\n",
    "\n",
    "fp = feature_pipeline.fit_transform(dp)\n",
    "df = fp\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/augustmeyers/Coding/jf-prediction/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['02ba5b48b116fae043ee650975502321', '05714b223e1005245ea6f18c75e15aaa', '150da79983f513a6160a68f55c246782', '15806488310742cdc6a68512e6897429', '1ce3247997a2f070c0178b52045465ca', '1f527828d94df81e3e04a69414a8b933', '217f31d93e654d035ebfb808f636bcc7', '261e63c5d1709bf3d9252c7641588f92', '26c55e4a7eb29c367c6747b58fc391dc', '2d272ea2e0dda0e3fe7b95b6657d6eb1', '2fb60beeef29557691d86f22457f7746', '359f66df2767b3f9d2b0c143a7323e83', '3717c3f837f3790440c192940233223b', '3a8bf953aa77bc4cb2d646d046439665', '3e4696ed679943cb95978372eec370d0', '404c7b690ad9313b06cdd49b85075990', '412f1e1622dd9c1bef9b130d4609145f', '46169a1779e3f196313961223ae5ce6b', '4897ad4c69864c41062c339fc33de989', '4942495c1a2fcc745b4b7ed946ddd58a', '49dfcf5f1fc2381dd2cdbaebea7c6321', '4fa668824c1769c3efec7c1b43d71810', '599587b7eaa55907aa50ddf5718c8e8c', '5a51bf8b783b273bdff285aea68a8580', '5a573538908d30cbb626a5dfac08c57a', '5bc8d6ffa74ae2b3f28a0b4fd6136579', '5c0674a3b14765c57c3b64c0de09baab', '5ecca21f396d2985273c111760d5bf90', '5f1c5fa00091b6d7ed596794f27cc53c', '6482027b86b7c37050cccffbb48ad5d4', '6807051adb93d214100b85bbf9a74e75', '6a0475014c5c234223f95fde9bfae03d', '6a1ae721bcb0fba0ad7a735550b9f24c', '6f53e2a97137ea3a6db0fd6e1931753d', '7113db1061fb50f371fe0fd95812e97d', '72e5d35dd3f84da0f2e9303800946463', '75c5d4528beb5c7d4bb89b9f72f695fe', '7e6cb7753cb477a117e67ea1e504fd70', '80ae90c0629e00f2b24f51c18b5b308c', '839d177e213045ac05f70dd0ee5658db', '84fefbd18b02f1760d19b260b9da4ccc', '85f7b8b4672f4eec45f3b4d6bec1858e', '86f3ac03cf67fe465094a48dd6fb80a3', '87e80812b7280cad731bb29d8fdac97b', '88377fbe6de5ae2c05eb50e5ce1fe5b5', '889711f02fea804d40a3d4b15a36c5cb', '89c18a61fa6b6c91cce44a6a57841d0e', '8bdc19cb55e3bfcd10fe9595615fc15b', '9609ba72f308b76b17aef3427912e71f', '96f8c87fd920563605b5c85ffaf35506', '97fba4423b7cd894f651fab827428f66', '9ba5194c0a681d80a347bee2dd1f4614', 'a030f0a68f6619af196309d97f7c61ce', 'a4e4a212a99c4db765dd1193488eb452', 'a5a9f3b394d3c3d1ef3a940b06caa3f2', 'a781c1af6567ab039303d03ffaff3371', 'a97d697ac20f91062113c507558563f3', 'aebd1f9b264c47145a937508e158b60a', 'afd168216e9777409b7572a55250ef6b', 'b158abb2240d8eece23ccda16d7f1027', 'b19edbd369346622b29ecb624340d54f', 'b7d8e92246b3dc9917e8236128738598', 'be5b30d14b00f154d26573a8f65198ea', 'beaee7ae1e30eeda0a1798a2594b3d3f', 'c10d99651bef377b7550f9b890fa10c1', 'c5e42859894627a4ffe058ea1ae650f4', 'cacb243beecce6f607f45c2f283483a4', 'cbad339e335e900bbe687c7f3c96477b', 'cf95ffc9f4aac8a6c602969950ac24ca', 'd4fd7d1602f47a74ee92ed8a1e9d1159', 'd98531d84edf06848235404cf7fa60e2', 'e0ede3dcca1ae537c3ae5dc4a937abe4', 'e2ed428c8f634071b600b71f09ba651e', 'e491f5a75eea7e823fa6de92fa424bfd', 'e6a18b22addef142ba7121375bc1e977', 'ebbcb0930a364aaf7e3919d8e732339b', 'ed2ef5068deb77c8b3ac3bd1f621590d', 'f1ebda05aad1416143548513d871eb41', 'f6d2d2dcae12c6bd820bd44619b52863', 'fe02268a343ca14dff08c2d62b7d72e7'] will be ignored\n",
      "  warnings.warn(\n",
      "/Users/augustmeyers/Coding/jf-prediction/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=2000; total time=   2.0s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=4000; total time=   3.0s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=4000; total time=   3.1s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=4000; total time=   3.1s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=2000; total time=   2.4s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=2000; total time=   2.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=4000; total time=   3.1s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=4000; total time=   3.3s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.4s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=2000; total time=   2.2s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=2000; total time=   2.5s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=2000; total time=   2.9s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=2000; total time=   3.0s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=2000; total time=   3.0s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=2000; total time=   3.1s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=2000; total time=   3.1s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=None, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.2s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=None, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.4s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=None, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.7s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.2s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.2s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=None, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.5s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=None, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.9s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.2s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.2s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=  10.7s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=  11.8s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=  12.5s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=  13.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=  13.5s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=4000; total time=  52.0s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=4000; total time=  53.8s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=4000; total time=  55.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=10, n_estimators=4000; total time=  50.6s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=10, n_estimators=4000; total time=  51.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=4000; total time=  55.9s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=4000; total time=  57.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=10, n_estimators=4000; total time=  52.2s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.6s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.7s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.6s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=4000; total time=   5.0s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.6s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=4000; total time=   5.0s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time=   0.9s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time=   0.9s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=4000; total time=   5.1s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=4000; total time=   4.9s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=4000; total time=   4.9s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time=   1.1s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time=   0.9s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time=   0.9s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=10, n_estimators=4000; total time=  36.9s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=10, n_estimators=4000; total time=  37.0s\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'gini', 'bootstrap': False}\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.7500\n",
      "Precision: 0.7037\n",
      "Recall: 0.5278\n",
      "F1 Score: 0.6032\n",
      "ROC-AUC: 0.8082\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.88      0.82        64\n",
      "        True       0.70      0.53      0.60        36\n",
      "\n",
      "    accuracy                           0.75       100\n",
      "   macro avg       0.74      0.70      0.71       100\n",
      "weighted avg       0.74      0.75      0.74       100\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56  8]\n",
      " [17 19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('feature_engineer', MovieFeatureEngineer()),\n",
    "])\n",
    "\n",
    "# Split data\n",
    "df = dp.drop(columns=['IsFavorite'])\n",
    "y = dp['IsFavorite']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply pipeline and resampling\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)  # Use transform instead of fit_transform for test data\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [200, 500, 1000, 2000, 4000],          # Number of trees in the forest\n",
    "    'max_depth': [10, 20, 50, None],           # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],           # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],             # Minimum samples required at each leaf node\n",
    "    'max_features': ['sqrt', 'log2', None],    # Number of features considered for the best split\n",
    "    'bootstrap': [True, False],                # Use bootstrap samples\n",
    "    'criterion': ['gini', 'entropy'],          # Split quality criterion\n",
    "}\n",
    "\n",
    "# Model and scoring\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    scoring=recall_scorer,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Best model\n",
    "tuned_rf = random_search.best_estimator_\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Predict probabilities for AUC evaluation\n",
    "y_pred_proba = tuned_rf.predict_proba(X_test_transformed)[:, 1]\n",
    "y_pred = tuned_rf.predict(X_test_transformed)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
