{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZX3wSrOWe3q",
        "outputId": "72aafa16-65c5-45d2-ecb0-171807c12277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 4)) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: fasttext-wheel in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 6)) (0.9.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2022.6)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fasttext-wheel->-r requirements.txt (line 6)) (63.2.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fasttext-wheel->-r requirements.txt (line 6)) (2.13.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\augus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QpB6oFgIWD8i"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv(\".env\")\n",
        "\n",
        "URL = os.getenv(\"URL\")\n",
        "USER_ID = os.getenv(\"USER_ID\")\n",
        "# USER_ID = \"d7252e24a2a34cab83e147e26fcee5d8\"\n",
        "\n",
        "API_KEY = os.getenv(\"API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "OQNfw7fuWD8k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import requests\n",
        "\n",
        "columns_to_keep = [\n",
        "    \"Name\", \"PremiereDate\", \"CriticRating\", \"OfficialRating\", \"Overview\", \"Taglines\",\n",
        "    \"Genres\", \"CommunityRating\", \"RunTimeTicks\", \"ProductionYear\", \"People\", \"Studios\", \"UserData\"\n",
        "]\n",
        "\n",
        "class JellyfinClient(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, url, user_id, api_key, played_status=\"IsPlayed\", limit=None):\n",
        "        \"\"\"\n",
        "        Initializes the Jellyfin Client.\n",
        "        :param url: Base URL of the Jellyfin server.\n",
        "        :param user_id: User ID for the API request.\n",
        "        :param api_key: API key for authentication.\n",
        "        :param played_status: \"IsPlayed\" or \"IsNotPlayed\" to filter movies.\n",
        "        \"\"\"\n",
        "        self.url = url\n",
        "        self.user_id = user_id\n",
        "        self.api_key = api_key\n",
        "        self.played_status = played_status  # Switch for IsPlayed or IsNotPlayed\n",
        "        self.limit = limit\n",
        "\n",
        "    def get_amount(self):\n",
        "        \"\"\"Fetch the total number of movies with the current filter.\"\"\"\n",
        "        res = requests.get(\n",
        "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
        "            f\"?StartIndex=0&Limit=1&Recursive=true&IncludeItemTypes=Movie\"\n",
        "            f\"&api_key={self.api_key}&Filters={self.played_status}\"\n",
        "        )\n",
        "        res.raise_for_status()\n",
        "        return res.json().get(\"TotalRecordCount\")\n",
        "\n",
        "    def get_chunk(self, start, chunk):\n",
        "        \"\"\"Fetch a chunk of movies with the current filter.\"\"\"\n",
        "        res = requests.get(\n",
        "            f\"{self.url}/emby/Users/{self.user_id}/Items\"\n",
        "            f\"?StartIndex={start}&Limit={chunk}&Recursive=true&IncludeItemTypes=Movie\"\n",
        "            f\"&api_key={self.api_key}&Filters={self.played_status}\"\n",
        "            f\"&Fields=Budget,Genres,Overview,People,Revenue,Studios,Taglines,ProviderIds,\"\n",
        "            f\"CriticRating,OfficialRating,PremiereDate,CommunityRating,RunTimeTicks,ProductionYear,UserData\"\n",
        "        )\n",
        "        res.raise_for_status()\n",
        "        return res.json().get(\"Items\")\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X=None):\n",
        "        \"\"\"Fetch all movies in chunks based on the current filter.\"\"\"\n",
        "        size = self.limit or self.get_amount()\n",
        "\n",
        "        start = 0\n",
        "        chunk = 50\n",
        "        all_movies = []\n",
        "        while start < size:\n",
        "            print(\n",
        "                f\"Fetching movies {start} - {start + chunk} of {size} ({self.played_status})\")\n",
        "            all_movies.extend(self.get_chunk(start, chunk))\n",
        "            start += chunk\n",
        "        return all_movies\n",
        "\n",
        "\n",
        "class MovieDataCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns_to_keep):\n",
        "        self.columns_to_keep = columns_to_keep\n",
        "\n",
        "    def filter_movie(self, movie):\n",
        "        \"\"\"Filter and clean a single movie dictionary.\"\"\"\n",
        "        filtered = {key: value for key,\n",
        "                    value in movie.items() if key in self.columns_to_keep}\n",
        "\n",
        "        # Process nested People and Studios\n",
        "        if \"People\" in filtered:\n",
        "            filtered[\"People\"] = [[person.get(\"Name\"), person.get(\n",
        "                \"Type\")] for person in filtered[\"People\"]]\n",
        "        if \"Studios\" in filtered:\n",
        "            filtered[\"Studios\"] = [studio[\"Name\"]\n",
        "                                   for studio in filtered[\"Studios\"] if \"Name\" in studio]\n",
        "        if \"UserData\" in filtered:\n",
        "            filtered[\"IsFavorite\"] = filtered[\"UserData\"].get(\"IsFavorite\")\n",
        "            del filtered[\"UserData\"]\n",
        "\n",
        "        if \"Taglines\" in filtered:\n",
        "            filtered[\"Taglines\"] = (\"\\n\").join(filtered[\"Taglines\"])\n",
        "\n",
        "        # Convert from Microseconds\n",
        "        if \"RunTimeTicks\" in filtered:\n",
        "            filtered[\"LengthInHours\"] = filtered[\"RunTimeTicks\"] / 10000000 / 60 / 60\n",
        "            del filtered[\"RunTimeTicks\"]\n",
        "            \n",
        "        return filtered\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return pd.DataFrame([self.filter_movie(movie) for movie in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6rSZE9zWD8l",
        "outputId": "4e8d31e0-9aa3-479d-dcf2-ecc2381a1612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching movies 0 - 50 of 539 (IsPlayed)\n",
            "Fetching movies 50 - 100 of 539 (IsPlayed)\n",
            "Fetching movies 100 - 150 of 539 (IsPlayed)\n",
            "Fetching movies 150 - 200 of 539 (IsPlayed)\n",
            "Fetching movies 200 - 250 of 539 (IsPlayed)\n",
            "Fetching movies 250 - 300 of 539 (IsPlayed)\n",
            "Fetching movies 300 - 350 of 539 (IsPlayed)\n",
            "Fetching movies 350 - 400 of 539 (IsPlayed)\n",
            "Fetching movies 400 - 450 of 539 (IsPlayed)\n",
            "Fetching movies 450 - 500 of 539 (IsPlayed)\n",
            "Fetching movies 500 - 550 of 539 (IsPlayed)\n"
          ]
        }
      ],
      "source": [
        "data_pipeline = Pipeline([\n",
        "    (\"jellyfin_client\", JellyfinClient(URL, USER_ID, API_KEY, played_status=\"IsPlayed\")),\n",
        "    (\"data_cleaner\", MovieDataCleaner(columns_to_keep)),\n",
        "])\n",
        "\n",
        "dp = data_pipeline.fit_transform(None)\n",
        "# dp.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jREfwvQVWD8l",
        "outputId": "9915bde0-d0c7-4846-a7da-0af65e230fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column: Name has null: False\n",
            "Column: PremiereDate has null: False\n",
            "Column: CriticRating has null: True\n",
            "Column: OfficialRating has null: True\n",
            "Column: Overview has null: False\n",
            "Column: Taglines has null: False\n",
            "Column: Genres has null: False\n",
            "Column: CommunityRating has null: True\n",
            "Column: ProductionYear has null: False\n",
            "Column: People has null: False\n",
            "Column: Studios has null: False\n",
            "Column: IsFavorite has null: False\n",
            "Column: LengthInHours has null: True\n"
          ]
        }
      ],
      "source": [
        "dp.columns\n",
        "for col in dp.columns:\n",
        "    print(f'Column: {col} has null: {dp[col].isnull().values.any()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dp.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "gf8k_ylEWD8m"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, OneHotEncoder\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "class MovieFeatureEngineerWithFastText(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100):\n",
        "        self.vector_size = vector_size\n",
        "        self.fasttext_combined_model = None\n",
        "        # Encoders and scalers\n",
        "        self.ohe_rating = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
        "        self.mlb_genres = MultiLabelBinarizer()\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def load_pretrained_fasttext_model(self):\n",
        "        \"\"\"\n",
        "        Load a pre-trained FastText model.\n",
        "        \"\"\"\n",
        "        import fasttext.util\n",
        "        fasttext.util.download_model('en', if_exists='ignore')  # Downloads Common Crawl vectors\n",
        "        model = fasttext.load_model('cc.en.300.bin')\n",
        "        return model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the feature engineering pipeline.\n",
        "        \"\"\"\n",
        "        # Fit numerical scaler\n",
        "        X_filled = X[['CriticRating', 'CommunityRating', 'LengthInHours', 'ProductionYear']].fillna(-7)\n",
        "        self.scaler.fit(X_filled)\n",
        "\n",
        "        # Load the pre-trained FastText model\n",
        "        self.fasttext_combined_model = self.load_pretrained_fasttext_model()\n",
        "\n",
        "        # Fit encoders for categorical and multilabel features\n",
        "        self.ohe_rating.fit(X[['OfficialRating']].fillna(\"unknown\"))\n",
        "        self.mlb_genres.fit(X['Genres'].fillna(\"unknown\"))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the input DataFrame into feature matrix.\n",
        "        \"\"\"\n",
        "        df = X.copy()\n",
        "\n",
        "        # Transform numerical features\n",
        "        numerical_features = ['CriticRating', 'CommunityRating', 'LengthInHours', 'ProductionYear']\n",
        "        df[numerical_features] = df[numerical_features].fillna(-7)\n",
        "        scaled_numerical = csr_matrix(self.scaler.transform(df[numerical_features]))\n",
        "\n",
        "        # Helper function for FastText embeddings\n",
        "        def get_fasttext_vector(text, model):\n",
        "            if isinstance(text, str) and text.strip():\n",
        "                vectors = [model.get_word_vector(word) for word in text.split() if word in model.words]\n",
        "                if vectors:\n",
        "                    return np.mean(vectors, axis=0)\n",
        "            return np.zeros(model.get_dimension())\n",
        "\n",
        "        # Combine text data for embedding generation\n",
        "        combined_text = (\n",
        "            df['Name'].fillna(\"\").astype(str) + \" \" +\n",
        "            df['Overview'].fillna(\"\").astype(str) + \" \" +\n",
        "            df['People'].apply(\n",
        "                lambda p: ' '.join([str(f\"{item[0]}, {item[1]}\") for item in p if isinstance(item, list) and len(item) > 0])\n",
        "            ).fillna(\"\") + \" \" +\n",
        "            df['Studios'].fillna(\"\").astype(str)\n",
        "        )\n",
        "\n",
        "        combined_embeddings = np.vstack(combined_text.apply(lambda x: get_fasttext_vector(x, self.fasttext_combined_model)))\n",
        "        combined_embeddings_sparse = csr_matrix(combined_embeddings)\n",
        "\n",
        "        # Count role occurrences\n",
        "        def count_roles(people):\n",
        "            role_counter = Counter()\n",
        "            for person in people:\n",
        "                if isinstance(person, list) and len(person) > 1:\n",
        "                    role_counter[person[1]] += 1\n",
        "            return role_counter\n",
        "\n",
        "        role_counts = df['People'].apply(count_roles).fillna(Counter())\n",
        "        role_features = csr_matrix(np.array([\n",
        "            [\n",
        "                role_counts.get('Actor', -7),\n",
        "                role_counts.get('Director', -7),\n",
        "                role_counts.get('Writer', -7),\n",
        "                role_counts.get('Producer', -7)\n",
        "            ]\n",
        "            for role_counts in role_counts\n",
        "        ]))\n",
        "\n",
        "        # Date encoding\n",
        "        df[\"PremiereDate\"] = pd.to_datetime(df['PremiereDate'], errors='coerce')\n",
        "        df['year'] = df['PremiereDate'].dt.year.fillna(-7).astype(int)\n",
        "        df['month'] = df['PremiereDate'].dt.month.fillna(-7).astype(int)\n",
        "        df['day'] = df['PremiereDate'].dt.day.fillna(-7).astype(int)\n",
        "        df['day_of_week'] = df['PremiereDate'].dt.dayofweek.fillna(-7).astype(int)\n",
        "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
        "        df['week_of_year'] = df['PremiereDate'].dt.isocalendar().week.fillna(-7).astype(int)\n",
        "        df['day_of_year'] = df['PremiereDate'].dt.dayofyear.fillna(-7).astype(int)\n",
        "        df = df.drop(columns=['PremiereDate'])\n",
        "        date_features = csr_matrix(df[['year', 'month', 'day', 'is_weekend', 'week_of_year', 'day_of_year']].values)\n",
        "\n",
        "        # Encode categorical and multilabel features\n",
        "        official_rating_encoded = self.ohe_rating.transform(df[['OfficialRating']].fillna(\"unknown\"))\n",
        "        genres_encoded = csr_matrix(self.mlb_genres.transform(df['Genres'].fillna(\"unknown\")))\n",
        "\n",
        "        # Stack all features together\n",
        "        final_sparse_matrix = hstack([\n",
        "            scaled_numerical,               # Scaled numerical data\n",
        "            combined_embeddings_sparse,     # Unified FastText embeddings\n",
        "            role_features,                  # Role count features\n",
        "            date_features,                  # Date encoding features\n",
        "            official_rating_encoded,        # One-hot encoded Official Rating\n",
        "            genres_encoded                  # Multi-label binarized Genres\n",
        "        ])\n",
        "\n",
        "        return final_sparse_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foCL4Zc0WD8m",
        "outputId": "6473790f-dccf-4c27-cd77-8046c96c6097"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Full pipeline\n",
        "full_pipeline = Pipeline([\n",
        "    ('feature_engineer', MovieFeatureEngineerWithFastText()),\n",
        "])\n",
        "\n",
        "# Split data\n",
        "df = dp.drop(columns=['IsFavorite'])\n",
        "y = dp['IsFavorite']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply pipeline and resampling\n",
        "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
        "X_test_transformed = full_pipeline.transform(X_test)  # Use transform instead of fit_transform for test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I22g_k4ecaWN",
        "outputId": "817a9dfc-1072-44fd-886b-d31a50ae0b4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(431, 345)"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_transformed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN6az3JsWD8m",
        "outputId": "4ef4cdf8-7813-4607-bc7a-44205d41d05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "Best Parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 10}\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.6204\n",
            "Precision: 0.6000\n",
            "Recall: 0.6792\n",
            "F1 Score: 0.6372\n",
            "ROC-AUC: 0.6758\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.65      0.56      0.60        55\n",
            "        True       0.60      0.68      0.64        53\n",
            "\n",
            "    accuracy                           0.62       108\n",
            "   macro avg       0.62      0.62      0.62       108\n",
            "weighted avg       0.62      0.62      0.62       108\n",
            "\n",
            "Confusion Matrix:\n",
            "[[31 24]\n",
            " [17 36]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': [200, 500, 1000],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}\n",
        "\n",
        "# Model and scoring\n",
        "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
        "recall_scorer = make_scorer(recall_score)\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    scoring=recall_scorer,\n",
        "    n_iter=20,\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Best model\n",
        "tuned_rf = random_search.best_estimator_\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Predict probabilities for AUC evaluation\n",
        "y_pred_proba = tuned_rf.predict_proba(X_test_transformed)[:, 1]\n",
        "y_pred = tuned_rf.predict(X_test_transformed)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbT-BF3HWD8n",
        "outputId": "1dd28f61-0a20-4557-9ab6-6cbc9bd30ad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching movies 0 - 50 of 2318 (IsUnPlayed)\n",
            "Fetching movies 50 - 100 of 2318 (IsUnPlayed)\n",
            "Fetching movies 100 - 150 of 2318 (IsUnPlayed)\n",
            "Fetching movies 150 - 200 of 2318 (IsUnPlayed)\n",
            "Fetching movies 200 - 250 of 2318 (IsUnPlayed)\n",
            "Fetching movies 250 - 300 of 2318 (IsUnPlayed)\n",
            "Fetching movies 300 - 350 of 2318 (IsUnPlayed)\n",
            "Fetching movies 350 - 400 of 2318 (IsUnPlayed)\n",
            "Fetching movies 400 - 450 of 2318 (IsUnPlayed)\n",
            "Fetching movies 450 - 500 of 2318 (IsUnPlayed)\n",
            "Fetching movies 500 - 550 of 2318 (IsUnPlayed)\n",
            "Fetching movies 550 - 600 of 2318 (IsUnPlayed)\n",
            "Fetching movies 600 - 650 of 2318 (IsUnPlayed)\n",
            "Fetching movies 650 - 700 of 2318 (IsUnPlayed)\n",
            "Fetching movies 700 - 750 of 2318 (IsUnPlayed)\n",
            "Fetching movies 750 - 800 of 2318 (IsUnPlayed)\n",
            "Fetching movies 800 - 850 of 2318 (IsUnPlayed)\n",
            "Fetching movies 850 - 900 of 2318 (IsUnPlayed)\n",
            "Fetching movies 900 - 950 of 2318 (IsUnPlayed)\n",
            "Fetching movies 950 - 1000 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1000 - 1050 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1050 - 1100 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1100 - 1150 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1150 - 1200 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1200 - 1250 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1250 - 1300 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1300 - 1350 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1350 - 1400 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1400 - 1450 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1450 - 1500 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1500 - 1550 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1550 - 1600 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1600 - 1650 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1650 - 1700 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1700 - 1750 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1750 - 1800 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1800 - 1850 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1850 - 1900 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1900 - 1950 of 2318 (IsUnPlayed)\n",
            "Fetching movies 1950 - 2000 of 2318 (IsUnPlayed)\n",
            "Fetching movies 2000 - 2050 of 2318 (IsUnPlayed)\n",
            "Fetching movies 2050 - 2100 of 2318 (IsUnPlayed)\n",
            "Fetching movies 2100 - 2150 of 2318 (IsUnPlayed)\n",
            "Fetching movies 2150 - 2200 of 2318 (IsUnPlayed)\n",
            "Fetching movies 2200 - 2250 of 2318 (IsUnPlayed)\n",
            "Fetching movies 2250 - 2300 of 2318 (IsUnPlayed)\n",
            "Fetching movies 2300 - 2350 of 2318 (IsUnPlayed)\n"
          ]
        }
      ],
      "source": [
        "new_pipeline = Pipeline([\n",
        "    (\"jellyfin_client\", JellyfinClient(URL, USER_ID, API_KEY, played_status=\"IsUnPlayed\")),\n",
        "    (\"data_cleaner\", MovieDataCleaner(columns_to_keep)),\n",
        "\n",
        "])\n",
        "\n",
        "new_df = new_pipeline.fit_transform(None)\n",
        "X_new_transformed = full_pipeline.transform(new_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xyfcLMcWD8n",
        "outputId": "b01b63f4-17f1-4c2b-eb0b-b163976ff215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hundreds of Beavers: 0.74\n",
            "Transformers: Rise of the Beasts: 0.73\n",
            "The Crow: 0.71\n",
            "The Creator: 0.69\n",
            "Jeepers Creepers 3: 0.69\n",
            "The Thing: 0.33\n",
            "Species III: 0.33\n",
            "Drag Me to Hell: 0.34\n",
            "Flight: 0.34\n",
            "For Your Eyes Only: 0.34\n"
          ]
        }
      ],
      "source": [
        "predictions = tuned_rf.predict(X_new_transformed)\n",
        "predicted_probabilities = tuned_rf.predict_proba(X_new_transformed)[:, 1]\n",
        "\n",
        "results_df = new_df[['Name']].copy()  # or 'MovieID', if you have it\n",
        "results_df['PredictedIsFavorite'] = predictions\n",
        "results_df['ProbabilityIsFavorite'] = predicted_probabilities\n",
        "\n",
        "# Top 5 highest\n",
        "top_5 = results_df.nlargest(5, 'ProbabilityIsFavorite')\n",
        "\n",
        "# Bottom 5 lowest\n",
        "bottom_5 = results_df.nsmallest(5, 'ProbabilityIsFavorite')\n",
        "\n",
        "# Combine the two lists and format the output\n",
        "output = \"\\n\".join(\n",
        "    f\"{row['Name']}: {row['ProbabilityIsFavorite']:.2f}\"\n",
        "    for _, row in pd.concat([top_5, bottom_5]).iterrows()\n",
        ")\n",
        "\n",
        "print(output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
